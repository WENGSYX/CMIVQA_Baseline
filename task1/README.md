# CMIVQA Track 1 Baseline



## Updates

- 2023/4/10 updates codes ğŸ†

## Main Method

![main](./image/main.png)

###### We introduce a novel task, named video corpus visual answer localization (VCVAL), which aims to locate the visual answer in a large collection of untrimmed, unsegmented instructional videos using a natural language question. This task requires a range of skills - the interaction between vision and language, video retrieval, passage comprehension, and visual answer localization. To solve these, we propose a cross-modal contrastive global-span (CCGS) method for the VCVAL, jointly training the video corpus retrieval and visual answer localization tasks. More precisely, we enhance the video question-answer semantic by adding element-wise visual information into the pre-trained language model, and designing a novel global-span predictor through fusion information to locate the visual answer point. The Global-span contrastive learning is adopted to differentiate the span point in the positive and negative samples with the global-span matrix. We have reconstructed a new dataset named MedVidCQA and benchmarked the VCVAL task, where the proposed method achieves state-of-the-art (SOTA) both in the video corpus retrieval and visual answer localization tasks.

## ç¯å¢ƒå®‰è£…

- python 3.7 with pytorch (`1.10.0`), transformers(`4.15.0`), tqdm, accelerate, pandas, numpy, glob, sentencepiece
- cuda10/cuda11

#### Installing the GPU driver

```shell script
# preparing environment
sudo apt-get install gcc
sudo apt-get install make
wget https://developer.download.nvidia.com/compute/cuda/11.5.1/local_installers/cuda_11.5.1_495.29.05_linux.run
sudo sh cuda_11.5.1_495.29.05_linux.run
```

#### Installing Conda and Python

```shell script
# preparing environment
wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
sudo chmod 777 Miniconda3-latest-Linux-x86_64.sh 
bash Miniconda3-latest-Linux-x86_64.sh

conda create -n CCGS python==3.7
conda activate CCGS
```

#### Installing Python Libraries

```plain
# preparing environment
pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
pip install tqdm transformers sklearn pandas numpy glob accelerate sentencepiece
```

## æ•°æ®

è¯·ä»[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1VRJZaQyGn5PbyGt0yVo1Gg?pwd=9874)æˆ–è€…[GoogleDrive](https://drive.google.com/drive/folders/1QbY8DEaVLkY2w6vOCWAs4ZQFHgJ3q8ui?usp=sharing)ä¸­ä¸‹è½½æ•°æ®ï¼Œå¹¶ä»¥ä»¥ä¸‹æ ¼å¼æ”¾ç½®äºæœ¬åœ°æ–‡ä»¶çš„NLPCC_2023_CMIVQA_TRAIN_DEVä¸­ï¼š

å…¶ä¸­çš„subtitle.jsonæ˜¯å¤„ç†å®Œæˆçš„æ–‡ä»¶ï¼Œè¯·ä»[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1ZpicCahLs-DwKQHJW7gz-A?pwd=1234 )ä¸‹è½½

```plain
-- NLPCC_2023_CMIVQA_TRAIN_DEV
  -- CMIVQA_Train_Dev.json
  -- video_feature
  -- subtitle.json
```

è¿è¡Œæœ¬é¡¹ç›®ï¼Œéœ€è¦NLPCC_2023_CMIVQA_TRAIN_DEVä¸­çš„`CMIVQA_Train_Dev.json`æ–‡ä»¶ã€`video_feature`æ–‡ä»¶å¤¹å’Œ`subtitle.json`æ–‡ä»¶



### æµ‹è¯•æ•°æ®

```plain
-- NLPCC_2023_CMIVQA_TESTA
  -- dataset_testA_for_track23.json
  -- video_feature
  -- subtitle.json
```

è¿è¡Œæœ¬é¡¹ç›®ï¼Œéœ€è¦NLPCC_2023_CMIVQA_TESTAä¸­çš„`CMIVQA_Train_Dev.json`æ–‡ä»¶ã€`video_feature`æ–‡ä»¶å¤¹å’Œ`subtitle.json`æ–‡ä»¶



## å¿«é€Ÿå¼€å§‹

### è®­ç»ƒ

```shell script
python main.py
```

> ä½ å¯ä»¥è°ƒæ•´å…¶ä¸­çš„è¶…å‚æ•°ä»¥ç¡®ä¿æ­£å¸¸è¿è¡Œ

### æµ‹è¯•

```shell script
python test.py
```

> ä½ éœ€è¦ä»logä¸­åŠ è½½è®­ç»ƒå®Œæˆçš„æ¨¡å‹ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆç»“æœ



## æ€§èƒ½

#### Temporal Answer Grounding in Singe Video Track

| R@1, IoU=0.3 | R@1, IoU=0.5 | R@1, IoU=0.7 | mIoU(R@1) |
| ------------ | ------------ | ------------ | --------- |
| 56.71        | 40.65        | 23.577       | 39.97     |



## Cite
```
@article{weng2022visual,
  title={Visual Answer Localization with Cross-modal Mutual Knowledge Transfer},
  author={Weng, Yixuan and Li, Bin},
  journal={arXiv preprint arXiv:2210.14823},
  year={2022}
}
```
